# Phase 6: ãƒ†ã‚¹ãƒˆãƒ»å“è³ªä¿è¨¼ãƒ»é‹ç”¨æº–å‚™ (Week 12)

## ãƒ•ã‚§ãƒ¼ã‚ºæ¦‚è¦

- **æœŸé–“**: Week 12 (1é€±é–“)
- **ç›®æ¨™**: ã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªä¿è¨¼ã¨ãƒªãƒªãƒ¼ã‚¹æº–å‚™
- **æˆæœç‰©**: ãƒ†ã‚¹ãƒˆå®Œäº†ãƒ»å“è³ªä¿è¨¼ã€é‹ç”¨ç’°å¢ƒæ§‹ç¯‰ã€æœ¬ç•ªãƒªãƒªãƒ¼ã‚¹

## ã‚¿ã‚¹ã‚¯ä¸€è¦§

### TASK-038: å˜ä½“ãƒ†ã‚¹ãƒˆå®Œæˆ

- **èª¬æ˜**: å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å˜ä½“ãƒ†ã‚¹ãƒˆ
- **å„ªå…ˆåº¦**: High
- **è¦‹ç©å·¥æ•°**: 16h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-037

#### å®Ÿè£…å†…å®¹(TASK-038)

1. Repository ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ 90%ä»¥ä¸Š
2. Service ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ†ã‚¹ãƒˆå®Œæˆ
3. ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
4. ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ

#### Repository ãƒ†ã‚¹ãƒˆä¾‹

```rust
// tests/repositories/document_repository_test.rs
use sqlx::SqlitePool;
use crate::repositories::DocumentRepository;
use crate::models::{Document, CreateDocumentRequest};
use chrono::{NaiveDate, Utc};

#[sqlx::test]
async fn test_create_document_success(pool: SqlitePool) {
    // ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    setup_test_data(&pool).await;
    
    let repo = DocumentRepository::new(pool);
    let request = CreateDocumentRequest {
        number: Some("TEST-001".to_string()),
        title: "ãƒ†ã‚¹ãƒˆæ–‡æ›¸".to_string(),
        document_type_id: 1,
        business_number: Some("JOB-2024-001".to_string()),
        created_by: 1,
        created_date: NaiveDate::from_ymd_opt(2024, 12, 15).unwrap(),
        internal_external: Some("internal".to_string()),
        importance_class: Some("class2".to_string()),
        personal_info: Some("none".to_string()),
        notes: Some("ãƒ†ã‚¹ãƒˆç”¨æ–‡æ›¸".to_string()),
    };

    // ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    let result = repo.create(request).await;

    // æ¤œè¨¼
    assert!(result.is_ok());
    let document = result.unwrap();
    assert_eq!(document.title, "ãƒ†ã‚¹ãƒˆæ–‡æ›¸");
    assert_eq!(document.number, "TEST-001");
    assert!(document.id > 0);
}

#[sqlx::test]
async fn test_search_documents_with_filters(pool: SqlitePool) {
    // ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    setup_test_documents(&pool).await;
    
    let repo = DocumentRepository::new(pool);
    let filters = DocumentSearchFilters {
        title: Some("ä¼šè­°".to_string()),
        created_date_from: Some(NaiveDate::from_ymd_opt(2024, 12, 1).unwrap()),
        created_date_to: Some(NaiveDate::from_ymd_opt(2024, 12, 31).unwrap()),
        pagination: Pagination { offset: 0, limit: 10 },
        ..Default::default()
    };

    // ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    let result = repo.search(filters).await;

    // æ¤œè¨¼
    assert!(result.is_ok());
    let (documents, total) = result.unwrap();
    assert_eq!(documents.len(), 2);
    assert_eq!(total, 2);
    assert!(documents.iter().all(|d| d.title.contains("ä¼šè­°")));
}

#[sqlx::test]
async fn test_update_document_not_found(pool: SqlitePool) {
    let repo = DocumentRepository::new(pool);
    let update = UpdateDocumentRequest {
        title: Some("æ›´æ–°ã‚¿ã‚¤ãƒˆãƒ«".to_string()),
        ..Default::default()
    };

    // å­˜åœ¨ã—ãªã„IDã§æ›´æ–°è©¦è¡Œ
    let result = repo.update(999, update).await;

    // ã‚¨ãƒ©ãƒ¼ãŒè¿”ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    assert!(result.is_err());
    assert_matches!(result.unwrap_err(), DocumentError::NotFound { .. });
}

async fn setup_test_documents(pool: &SqlitePool) {
    sqlx::query!(
        r#"
        INSERT INTO documents (
            number, title, document_type_id, created_by, created_date, 
            internal_external, importance_class, personal_info, is_active
        ) VALUES 
        ('TEST-001', 'æœˆæ¬¡ä¼šè­°è­°äº‹éŒ²', 1, 1, '2024-12-15', 'internal', 'class2', 'none', 1),
        ('TEST-002', 'å®šä¾‹ä¼šè­°è³‡æ–™', 1, 1, '2024-12-14', 'internal', 'class2', 'none', 1),
        ('TEST-003', 'ææ¡ˆæ›¸', 2, 2, '2024-12-13', 'external', 'class1', 'present', 1)
        "#
    )
    .execute(pool)
    .await
    .unwrap();
}
```

#### Service ãƒ†ã‚¹ãƒˆä¾‹

```rust
// tests/services/document_service_test.rs
use mockall::predicate::*;
use crate::services::DocumentService;
use crate::repositories::MockDocumentRepository;
use crate::services::MockNumberGenerationService;

#[tokio::test]
async fn test_create_document_with_number_generation() {
    // ãƒ¢ãƒƒã‚¯è¨­å®š
    let mut mock_repo = MockDocumentRepository::new();
    let mut mock_number_service = MockNumberGenerationService::new();
    
    mock_number_service
        .expect_generate_number()
        .with(eq("T"), eq("A"), always())
        .times(1)
        .returning(|_, _, _| Ok("CTA-2508001".to_string()));
    
    mock_repo
        .expect_create()
        .with(always())
        .times(1)
        .returning(|req| {
            Ok(Document {
                id: 1,
                number: req.number.unwrap(),
                title: req.title,
                document_type_id: req.document_type_id,
                created_by: req.created_by,
                created_date: req.created_date,
                internal_external: req.internal_external,
                importance_class: req.importance_class,
                personal_info: req.personal_info,
                notes: req.notes,
                network_path: Some("\\\\server01\\docs\\2024\\æŠ€è¡“éƒ¨\\å ±å‘Šæ›¸".to_string()),
                is_active: true,
                created_at: Utc::now().naive_utc(),
                updated_at: Utc::now().naive_utc(),
            })
        });
    
    let service = DocumentService::new(
        Box::new(mock_repo), 
        Box::new(mock_number_service),
        Box::new(MockPathGenerationService::new())
    );
    
    let request = CreateDocumentRequest {
        number: None, // è‡ªå‹•ç”Ÿæˆ
        title: "ãƒ†ã‚¹ãƒˆæ–‡æ›¸".to_string(),
        document_type_id: 1,
        created_by: 1,
        created_date: NaiveDate::from_ymd_opt(2024, 12, 15).unwrap(),
        ..Default::default()
    };
    
    let user_permissions = UserPermissions::default();
    
    // ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    let result = service.create_document(request, &user_permissions).await;
    
    // æ¤œè¨¼
    assert!(result.is_ok());
    let document = result.unwrap();
    assert_eq!(document.number, "CTA-2508001");
    assert_eq!(document.title, "ãƒ†ã‚¹ãƒˆæ–‡æ›¸");
}

#[tokio::test]
async fn test_search_with_permission_filter() {
    let mut mock_repo = MockDocumentRepository::new();
    
    // æ¨©é™ã«åŸºã¥ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    mock_repo
        .expect_search_with_permissions()
        .with(always(), eq(vec![1, 2])) // ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½éƒ¨ç½²
        .times(1)
        .returning(|_, _| Ok((vec![/* æ¨©é™ã®ã‚ã‚‹æ–‡æ›¸ã®ã¿ */], 5)));
    
    let service = DocumentService::new(
        Box::new(mock_repo),
        Box::new(MockNumberGenerationService::new()),
        Box::new(MockPathGenerationService::new())
    );
    
    let user_permissions = UserPermissions {
        accessible_departments: vec![1, 2],
        can_view_confidential: false,
        ..Default::default()
    };
    
    // ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    let result = service.search_documents(
        DocumentSearchFilters::default(),
        &user_permissions
    ).await;
    
    // æ¤œè¨¼
    assert!(result.is_ok());
    // æ¨©é™ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
}
```

#### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®š

```bash
# ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
cargo install cargo-tarpaulin

# ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
cargo tarpaulin --out Html --output-dir coverage

# å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --lib

# ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ
cargo test repositories::document_repository
```

#### Property-based Testing

```rust
// tests/property/document_properties.rs
use proptest::prelude::*;
use crate::services::NumberGenerationService;
use crate::models::CreateDocumentRequest;

// æ–‡æ›¸ç•ªå·ç”Ÿæˆã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
proptest! {
    #[test]
    fn test_document_number_generation_properties(
        dept_code in "[A-Z]{1,3}",
        year in 2020u32..2030u32,
        sequence in 1u32..99999u32
    ) {
        let service = NumberGenerationService::new();
        let result = service.format_number(&dept_code, year, sequence);
        
        // ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£æ¤œè¨¼
        prop_assert!(result.len() <= 20, "Document number too long: {}", result);
        prop_assert!(result.contains(&dept_code), "Missing department code: {}", result);
        prop_assert!(result.contains(&year.to_string()[2..]), "Missing year: {}", result);
        prop_assert!(!result.contains(" "), "Contains spaces: {}", result);
        prop_assert!(result.chars().all(|c| c.is_alphanumeric() || c == '-'), "Invalid characters: {}", result);
    }
    
    #[test] 
    fn test_search_input_validation(
        title in "[\\w\\s\u3042-\u3093\u30a2-\u30f3\u4e00-\u9fff]{0,100}",
        limit in 1u32..1000u32,
        offset in 0u32..100000u32
    ) {
        let search_input = DocumentSearchInput {
            title: if title.trim().is_empty() { None } else { Some(title) },
            pagination: Pagination { limit, offset },
            ..Default::default()
        };
        
        let validation_result = validate_search_input(&search_input);
        
        // å…¥åŠ›æ¤œè¨¼ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
        prop_assert!(validation_result.is_ok(), "Valid input should pass validation");
        prop_assert!(search_input.pagination.limit > 0, "Limit must be positive");
        prop_assert!(search_input.pagination.limit <= 1000, "Limit must not exceed 1000");
    }
    
    #[test]
    fn test_network_path_generation_properties(
        document_number in "[A-Z]+-[0-9]{8}",
        year in 2020u32..2030u32,
        dept_name in "[\\w]{1,20}"
    ) {
        let path_service = PathGenerationService::new();
        let result = path_service.generate_path(&document_number, &dept_name, year);
        
        prop_assert!(result.is_ok(), "Path generation should succeed");
        
        if let Ok(path) = result {
            prop_assert!(path.starts_with("\\\\"), "Path should be UNC path: {}", path);
            prop_assert!(path.contains(&year.to_string()), "Path should contain year: {}", path);
            prop_assert!(path.contains(&dept_name), "Path should contain department: {}", path);
            prop_assert!(!path.contains("//"), "Path should not have double slashes: {}", path);
        }
    }
}

// ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
proptest! {
    #[test]
    fn test_sql_injection_resistance(
        malicious_input in ".*['\";<>\\\\]",
        table_name in "(users|documents|employees)",
        operation in "(SELECT|INSERT|UPDATE|DELETE)"
    ) {
        let repository = DocumentRepositoryImpl::new(test_pool());
        
        // æ‚ªæ„ã‚ã‚‹å…¥åŠ›ã§ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        let search_filters = DocumentSearchFilters {
            title: Some(malicious_input.clone()),
            business_number: Some(malicious_input),
            ..Default::default()
        };
        
        let result = repository.search_with_permissions(
            search_filters, 
            &UserPermissions::default()
        ).await;
        
        // SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãŒæˆåŠŸã—ãªã„ã“ã¨ã‚’ç¢ºèª
        prop_assert!(
            result.is_ok() || matches!(result, Err(DocumentError::Validation(_))),
            "Malicious input should be handled safely"
        );
    }
}
```

#### æˆæœç‰©(TASK-038)

- **90%ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**
- **å…¨Repositoryãƒ»Serviceãƒ†ã‚¹ãƒˆ**
- **ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ**
- **ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ**
- **ğŸš€ Property-based Testing**
- **ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ**

---

### TASK-039: çµ±åˆãƒ†ã‚¹ãƒˆ

- **èª¬æ˜**: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
- **å„ªå…ˆåº¦**: High
- **è¦‹ç©å·¥æ•°**: 12h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-038

#### å®Ÿè£…å†…å®¹(TASK-039)

1. APIçµ±åˆãƒ†ã‚¹ãƒˆ
2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
3. å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ é€£æºãƒ†ã‚¹ãƒˆ
4. ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

#### APIçµ±åˆãƒ†ã‚¹ãƒˆ

```rust
// tests/integration/api_integration_test.rs
use axum_test::TestServer;
use sqlx::SqlitePool;
use serde_json::json;

#[sqlx::test]
async fn test_document_crud_workflow(pool: SqlitePool) {
    // ãƒ†ã‚¹ãƒˆã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    let app = create_test_app(pool).await;
    let server = TestServer::new(app).unwrap();
    
    // èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
    let auth_token = get_test_auth_token(&server).await;
    
    // 1. æ–‡æ›¸ä½œæˆ
    let create_response = server
        .post("/graphql")
        .add_header("Authorization", format!("Bearer {}", auth_token))
        .json(&json!({
            "query": r#"
                mutation CreateDocument($input: CreateDocumentInput!) {
                    createDocument(input: $input) {
                        success
                        data {
                            id
                            number
                            title
                            networkPath
                        }
                        errors
                    }
                }
            "#,
            "variables": {
                "input": {
                    "title": "çµ±åˆãƒ†ã‚¹ãƒˆæ–‡æ›¸",
                    "documentTypeId": "1",
                    "createdDate": "2024-12-15",
                    "internalExternal": "INTERNAL",
                    "importanceClass": "CLASS2",
                    "personalInfo": "NONE"
                }
            }
        }))
        .await;
    
    create_response.assert_status_ok();
    let create_data: serde_json::Value = create_response.json();
    
    assert!(create_data["data"]["createDocument"]["success"].as_bool().unwrap());
    let document_id = create_data["data"]["createDocument"]["data"]["id"].as_str().unwrap();
    let document_number = create_data["data"]["createDocument"]["data"]["number"].as_str().unwrap();
    let network_path = create_data["data"]["createDocument"]["data"]["networkPath"].as_str().unwrap();
    
    // æ–‡æ›¸ç•ªå·å½¢å¼ã®æ¤œè¨¼
    assert!(document_number.len() > 0);
    assert!(network_path.starts_with("\\\\"));
    
    // 2. æ–‡æ›¸å–å¾—
    let get_response = server
        .post("/graphql")
        .add_header("Authorization", format!("Bearer {}", auth_token))
        .json(&json!({
            "query": r#"
                query GetDocument($id: ID!) {
                    document(id: $id) {
                        id
                        number
                        title
                        createdDate
                        creator {
                            name
                        }
                        documentType {
                            name
                        }
                    }
                }
            "#,
            "variables": { "id": document_id }
        }))
        .await;
    
    get_response.assert_status_ok();
    let get_data: serde_json::Value = get_response.json();
    
    assert_eq!(
        get_data["data"]["document"]["number"].as_str().unwrap(),
        document_number
    );
    assert_eq!(
        get_data["data"]["document"]["title"].as_str().unwrap(),
        "çµ±åˆãƒ†ã‚¹ãƒˆæ–‡æ›¸"
    );
    
    // 3. æ–‡æ›¸æ¤œç´¢
    let search_response = server
        .post("/graphql")
        .add_header("Authorization", format!("Bearer {}", auth_token))
        .json(&json!({
            "query": r#"
                query SearchDocuments($input: DocumentSearchInput!) {
                    searchDocuments(input: $input) {
                        documents {
                            id
                            number
                            title
                        }
                        totalCount
                        hasNextPage
                    }
                }
            "#,
            "variables": {
                "input": {
                    "title": "çµ±åˆãƒ†ã‚¹ãƒˆ",
                    "pagination": {
                        "offset": 0,
                        "limit": 10
                    }
                }
            }
        }))
        .await;
    
    search_response.assert_status_ok();
    let search_data: serde_json::Value = search_response.json();
    
    let documents = search_data["data"]["searchDocuments"]["documents"].as_array().unwrap();
    assert!(documents.len() >= 1);
    assert_eq!(documents[0]["number"].as_str().unwrap(), document_number);
    
    // 4. æ–‡æ›¸æ›´æ–°
    let update_response = server
        .post("/graphql")
        .add_header("Authorization", format!("Bearer {}", auth_token))
        .json(&json!({
            "query": r#"
                mutation UpdateDocument($id: ID!, $input: UpdateDocumentInput!) {
                    updateDocument(id: $id, input: $input) {
                        success
                        data {
                            title
                            notes
                        }
                        errors
                    }
                }
            "#,
            "variables": {
                "id": document_id,
                "input": {
                    "title": "æ›´æ–°ã•ã‚ŒãŸçµ±åˆãƒ†ã‚¹ãƒˆæ–‡æ›¸",
                    "notes": "çµ±åˆãƒ†ã‚¹ãƒˆã§æ›´æ–°ã•ã‚Œã¾ã—ãŸ"
                }
            }
        }))
        .await;
    
    update_response.assert_status_ok();
    let update_data: serde_json::Value = update_response.json();
    
    assert!(update_data["data"]["updateDocument"]["success"].as_bool().unwrap());
    assert_eq!(
        update_data["data"]["updateDocument"]["data"]["title"].as_str().unwrap(),
        "æ›´æ–°ã•ã‚ŒãŸçµ±åˆãƒ†ã‚¹ãƒˆæ–‡æ›¸"
    );
    
    // 5. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    let file_check_response = server
        .post("/api/files/check")
        .add_header("Authorization", format!("Bearer {}", auth_token))
        .json(&json!({
            "documentId": document_id.parse::<i32>().unwrap()
        }))
        .await;
    
    file_check_response.assert_status_ok();
    let file_check_data: serde_json::Value = file_check_response.json();
    
    // ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªçµæœã®æ¤œè¨¼ï¼ˆå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¨ãƒ©ãƒ¼ã§ãªã„ã“ã¨ã‚’ç¢ºèªï¼‰
    assert!(file_check_data.get("folderExists").is_some());
}

#[sqlx::test]
async fn test_permission_based_access_control(pool: SqlitePool) {
    let app = create_test_app(pool).await;
    let server = TestServer::new(app).unwrap();
    
    // åˆ¶é™ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ­ã‚°ã‚¤ãƒ³
    let limited_token = get_limited_user_token(&server).await;
    
    // ç®¡ç†è€…æ©Ÿèƒ½ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ
    let admin_response = server
        .post("/api/admin/users")
        .add_header("Authorization", format!("Bearer {}", limited_token))
        .await;
    
    admin_response.assert_status(StatusCode::FORBIDDEN);
    
    // æ©Ÿå¯†æ–‡æ›¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ
    let confidential_response = server
        .post("/graphql")
        .add_header("Authorization", format!("Bearer {}", limited_token))
        .json(&json!({
            "query": r#"
                query GetConfidentialDocument($id: ID!) {
                    document(id: $id) {
                        id
                        title
                    }
                }
            "#,
            "variables": { "id": "999" } // æ©Ÿå¯†æ–‡æ›¸ID
        }))
        .await;
    
    confidential_response.assert_status_ok();
    let data: serde_json::Value = confidential_response.json();
    
    // æ¨©é™ãŒãªã„å ´åˆã¯nullãŒè¿”ã•ã‚Œã‚‹
    assert!(data["data"]["document"].is_null());
}
```

#### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ

```rust
// tests/integration/transaction_test.rs
#[sqlx::test]
async fn test_document_creation_transaction_rollback(pool: SqlitePool) {
    let document_service = DocumentService::new(/* ... */);
    
    // æ­£å¸¸ãªã‚±ãƒ¼ã‚¹
    let valid_request = CreateDocumentRequest {
        title: "æœ‰åŠ¹ãªæ–‡æ›¸".to_string(),
        document_type_id: 1,
        created_by: 1,
        created_date: NaiveDate::from_ymd_opt(2024, 12, 15).unwrap(),
        ..Default::default()
    };
    
    let result1 = document_service.create_document(valid_request, &UserPermissions::admin()).await;
    assert!(result1.is_ok());
    
    // ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã‚±ãƒ¼ã‚¹ï¼ˆé‡è¤‡æ–‡æ›¸ç•ªå·ï¼‰
    let duplicate_request = CreateDocumentRequest {
        number: Some(result1.unwrap().number), // é‡è¤‡ç•ªå·
        title: "é‡è¤‡æ–‡æ›¸".to_string(),
        document_type_id: 1,
        created_by: 1,
        created_date: NaiveDate::from_ymd_opt(2024, 12, 15).unwrap(),
        ..Default::default()
    };
    
    let result2 = document_service.create_document(duplicate_request, &UserPermissions::admin()).await;
    assert!(result2.is_err());
    
    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒä¸€è²«ã—ãŸçŠ¶æ…‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    let search_result = document_service.search_documents(
        DocumentSearchFilters {
            title: Some("é‡è¤‡æ–‡æ›¸".to_string()),
            ..Default::default()
        },
        &UserPermissions::admin()
    ).await.unwrap();
    
    assert_eq!(search_result.0.len(), 0); // é‡è¤‡æ–‡æ›¸ã¯ä½œæˆã•ã‚Œã¦ã„ãªã„
}
```

#### æˆæœç‰©(TASK-039)

- APIçµ±åˆãƒ†ã‚¹ãƒˆ
- ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
- æ¨©é™åˆ¶å¾¡ãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

---

### TASK-040: æ€§èƒ½ãƒ†ã‚¹ãƒˆ

- **èª¬æ˜**: è² è·ãƒ†ã‚¹ãƒˆãƒ»æ€§èƒ½æ¸¬å®š
- **å„ªå…ˆåº¦**: High
- **è¦‹ç©å·¥æ•°**: 8h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-039

#### å®Ÿè£…å†…å®¹(TASK-040)

1. è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
4. æ€§èƒ½æ”¹å–„ææ¡ˆ

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

```rust
// benches/document_search_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use tokio::runtime::Runtime;

fn document_search_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let service = rt.block_on(create_test_document_service());
    
    // å¤§é‡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    rt.block_on(setup_large_dataset(10000)); // 1ä¸‡ä»¶ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    
    let mut group = c.benchmark_group("document_search");
    
    for record_count in [100, 1000, 5000, 10000].iter() {
        group.bench_with_input(
            BenchmarkId::new("search_by_title", record_count),
            record_count,
            |b, &record_count| {
                b.to_async(&rt).iter(|| async {
                    let filters = DocumentSearchFilters {
                        title: Some("ãƒ†ã‚¹ãƒˆ".to_string()),
                        pagination: Pagination { offset: 0, limit: 50 },
                        ..Default::default()
                    };
                    
                    let result = service.search_documents(
                        black_box(filters),
                        &UserPermissions::admin()
                    ).await;
                    
                    black_box(result);
                });
            },
        );
    }
    
    group.finish();
}

fn file_existence_check_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let service = rt.block_on(create_test_file_check_service());
    
    c.bench_function("file_existence_check_100", |b| {
        b.to_async(&rt).iter(|| async {
            let documents = get_test_documents(100).await;
            
            for document in black_box(documents) {
                let result = service.check_document_existence(&document).await;
                black_box(result);
            }
        })
    });
}

criterion_group!(benches, document_search_benchmark, file_existence_check_benchmark);
criterion_main!(benches);
```

#### è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª

```bash
# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# CPUãƒ»ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
cargo install flamegraph
cargo flamegraph --bench document_search_benchmark

# æ€§èƒ½æ¸¬å®šãƒ¬ãƒãƒ¼ãƒˆ
cargo bench -- --output-format html
```

#### æ€§èƒ½åŸºæº–

| æ©Ÿèƒ½         | ç›®æ¨™æ€§èƒ½   | æ¸¬å®šæ–¹æ³•                    |
| ------------ | ---------- | --------------------------- |
| æ–‡æ›¸æ¤œç´¢     | 2ç§’ä»¥å†…    | 1000ä»¶æ¤œç´¢ã§ã®å¹³å‡å¿œç­”æ™‚é–“  |
| æ–‡æ›¸ä½œæˆ     | 1ç§’ä»¥å†…    | æ–‡æ›¸ç•ªå·ç”Ÿæˆè¾¼ã¿ã®ä½œæˆæ™‚é–“  |
| ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª | 5ç§’ä»¥å†…    | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¾¼ã¿    |
| åŒæ™‚æ¥ç¶š     | 10ãƒ¦ãƒ¼ã‚¶ãƒ¼ | åŒæ™‚æ¤œç´¢ã§ã®æ€§èƒ½åŠ£åŒ–20%ä»¥å†… |

#### æˆæœç‰©(TASK-040)

- æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
- è² è·ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
- æ€§èƒ½æ”¹å–„ææ¡ˆ
- ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹

---

### TASK-041: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

- **èª¬æ˜**: APIä»•æ§˜ãƒ»é‹ç”¨æ‰‹é †æ›¸
- **å„ªå…ˆåº¦**: Medium
- **è¦‹ç©å·¥æ•°**: 12h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-040

#### å®Ÿè£…å†…å®¹(TASK-041)

1. APIä»•æ§˜æ›¸è‡ªå‹•ç”Ÿæˆ
2. é‹ç”¨æ‰‹é †æ›¸ä½œæˆ
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«
4. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

#### APIä»•æ§˜æ›¸ç”Ÿæˆ

```rust
// src/docs/api_docs.rs
use async_graphql::*;

// GraphQLã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰APIä»•æ§˜æ›¸ã‚’è‡ªå‹•ç”Ÿæˆ
pub fn generate_api_docs() -> String {
    let schema = Schema::build(QueryRoot, MutationRoot, EmptySubscription)
        .finish();
    
    schema.sdl()
}

// OpenAPIã‚¹ãƒšãƒƒã‚¯ç”Ÿæˆï¼ˆREST APIç”¨ï¼‰
pub fn generate_openapi_spec() -> utoipa::openapi::OpenApi {
    use utoipa::OpenApi;
    
    #[derive(OpenApi)]
    #[openapi(
        paths(
            health_check,
            file_check,
            batch_run
        ),
        components(
            schemas(FileCheckResponse, BatchExecutionResponse)
        ),
        tags(
            (name = "Health", description = "System health endpoints"),
            (name = "Files", description = "File operation endpoints"),
            (name = "Batch", description = "Batch processing endpoints")
        )
    )]
    struct ApiDoc;
    
    ApiDoc::openapi()
}
```

#### é‹ç”¨æ‰‹é †æ›¸

```markdown
  # é‹ç”¨æ‰‹é †æ›¸
  
  ## æ—¥æ¬¡é‹ç”¨
  
  ### ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç¢ºèª
  #### 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç¢ºèª
  
     ```bash
     curl http://localhost:8080/health
     ```
  
  #### 2. ãƒ­ã‚°ç¢ºèª
  
     ```bash
     tail -f logs/application.log
     ```
  
  #### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
  
     ```bash
     sqlx migrate info --database-url sqlite://./data/prod.db
     ```
  
  ## æœˆæ¬¡é‹ç”¨
  
  ### ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªãƒãƒƒãƒ
  
  - å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°: æ¯æœˆ1æ—¥ 9:00 è‡ªå‹•å®Ÿè¡Œ
  - æ‰‹å‹•å®Ÿè¡Œ:
  
    ```bash
    curl -X POST /api/admin/batch/run/file-check
    ```
  
  ### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
  
  1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
  
     ```bash
     cp data/prod.db backup/prod_$(date +%Y%m%d).db
     ```
  
  2. çµ±è¨ˆæƒ…å ±æ›´æ–°
  
     ```sql
     ANALYZE;
     ```
  
  ## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
  
  ### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼
  
  **ç—‡çŠ¶**: `Connection refused` ã‚¨ãƒ©ãƒ¼
  **åŸå› **: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒƒã‚¯ã¾ãŸã¯æ¨©é™å•é¡Œ
  **å¯¾å‡¦**:
  
  1. ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª: `ps aux | grep doc_man_db`
  2. ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ç¢ºèª: `ls -la data/prod.db`
  3. å¿…è¦ã«å¿œã˜ã¦ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•
  
  ### ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼
  
  **ç—‡çŠ¶**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼
  **åŸå› **: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–æ¥ç¶šå•é¡Œ
  **å¯¾å‡¦**:
  
  1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèª
  2. èªè¨¼æƒ…å ±ç¢ºèª
  3. ãƒ‘ã‚¹è¨­å®šç¢ºèª

```

#### æˆæœç‰©(TASK-041)

- APIä»•æ§˜æ›¸ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
- é‹ç”¨æ‰‹é †æ›¸
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

---

### TASK-042: ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š

- **èª¬æ˜**: æœ¬ç•ªç’°å¢ƒè¨­å®šãƒ»CI/CD
- **å„ªå…ˆåº¦**: Medium
- **è¦‹ç©å·¥æ•°**: 8h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-041

#### å®Ÿè£…å†…å®¹(TASK-042)

1. æœ¬ç•ªç’°å¢ƒè¨­å®š
2. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
3. ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
4. ç’°å¢ƒç›£è¦–è¨­å®š

#### GitHub Actions CI/CD

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Cache cargo
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Check formatting
      run: cargo fmt --all -- --check
    
    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings
    
    - name: Run tests
      run: cargo test --all-features
    
    - name: Run integration tests
      run: cargo test --test '*'
    
  build:
    needs: test
    runs-on: windows-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Build release
      run: cargo build --release
    
    - name: Create deployment package
      run: |
        mkdir deploy
        Copy-Item target/release/doc_man_db.exe deploy/
        Copy-Item -Recurse migrations deploy/
        Copy-Item .env.production deploy/.env
        Compress-Archive -Path deploy/* -DestinationPath doc_man_db.zip
    
    - name: Upload artifact
      uses: actions/upload-artifact@v3
      with:
        name: deployment-package
        path: doc_man_db.zip
  
  deploy:
    needs: build
    runs-on: windows-latest
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - name: Download artifact
      uses: actions/download-artifact@v3
      with:
        name: deployment-package
    
    - name: Deploy to production
      run: |
        # PowerShellãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
        ./scripts/deploy.ps1 -Package doc_man_db.zip -Environment production
```

#### ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```powershell
# scripts/deploy.ps1
param(
    [Parameter(Mandatory=$true)]
    [string]$Package,
    
    [Parameter(Mandatory=$true)]
    [ValidateSet("staging", "production")]
    [string]$Environment
)

$ErrorActionPreference = "Stop"

Write-Host "Starting deployment to $Environment"

# è¨­å®šèª­ã¿è¾¼ã¿
$config = Get-Content "deploy-config.json" | ConvertFrom-Json
$deployPath = $config.$Environment.deployPath
$serviceName = $config.$Environment.serviceName

try {
    # ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
    Write-Host "Stopping service: $serviceName"
    Stop-Service -Name $serviceName -Force -ErrorAction SilentlyContinue
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    $backupPath = "$deployPath\backup\$(Get-Date -Format 'yyyyMMdd_HHmmss')"
    Write-Host "Creating backup: $backupPath"
    if (Test-Path $deployPath) {
        Copy-Item -Path $deployPath -Destination $backupPath -Recurse
    }
    
    # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±•é–‹
    Write-Host "Extracting package: $Package"
    Expand-Archive -Path $Package -DestinationPath $deployPath -Force
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    Write-Host "Running database migrations"
    Set-Location $deployPath
    .\doc_man_db.exe migrate
    
    # ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
    Write-Host "Starting service: $serviceName"
    Start-Service -Name $serviceName
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    Write-Host "Performing health check"
    $healthUrl = $config.$Environment.healthUrl
    for ($i = 1; $i -le 30; $i++) {
        try {
            $response = Invoke-RestMethod -Uri $healthUrl -TimeoutSec 10
            if ($response -eq "OK") {
                Write-Host "Health check passed"
                break
            }
        }
        catch {
            if ($i -eq 30) {
                throw "Health check failed after 30 attempts"
            }
            Start-Sleep -Seconds 2
        }
    }
    
    Write-Host "Deployment completed successfully"
}
catch {
    Write-Error "Deployment failed: $_"
    
    # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if (Test-Path $backupPath) {
        Write-Host "Rolling back to previous version"
        Remove-Item -Path $deployPath -Recurse -Force
        Copy-Item -Path $backupPath -Destination $deployPath -Recurse
        Start-Service -Name $serviceName
    }
    
    exit 1
}
```

#### æˆæœç‰©(TASK-042)

- æœ¬ç•ªç’°å¢ƒè¨­å®š
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½

---

### TASK-043: é‹ç”¨ç›£è¦–è¨­å®š

- **èª¬æ˜**: ãƒ­ã‚°ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
- **å„ªå…ˆåº¦**: Medium
- **è¦‹ç©å·¥æ•°**: 6h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-042

#### å®Ÿè£…å†…å®¹(TASK-043)

1. ãƒ­ã‚°ç›£è¦–è¨­å®š
2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
3. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰

#### æˆæœç‰©(TASK-043)

- ãƒ­ã‚°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥
- é‹ç”¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

---

### TASK-044: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç ”ä¿®æº–å‚™

- **èª¬æ˜**: æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ»ç ”ä¿®è³‡æ–™
- **å„ªå…ˆåº¦**: Low
- **è¦‹ç©å·¥æ•°**: 8h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-043

#### æˆæœç‰©(TASK-044)

- æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«
- ç ”ä¿®è³‡æ–™
- FAQé›†

---

### TASK-045: æœ¬ç•ªãƒªãƒªãƒ¼ã‚¹

- **èª¬æ˜**: æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»æ¤œè¨¼
- **å„ªå…ˆåº¦**: High
- **è¦‹ç©å·¥æ•°**: 4h
- **çŠ¶æ…‹**: æœªç€æ‰‹
- **ä¾å­˜é–¢ä¿‚**: TASK-044

#### æˆæœç‰©(TASK-045)

- æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†
- æœ¬ç•ªå‹•ä½œæ¤œè¨¼
- ãƒªãƒªãƒ¼ã‚¹å®Œäº†å ±å‘Š

## ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†åŸºæº–

### å¿…é ˆæ¡ä»¶

- [ ] å˜ä½“ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ 90%ä»¥ä¸Š
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆå…¨ã¦é€šé
- [ ] æ€§èƒ½åŸºæº–ã‚’æº€ãŸã™
- [ ] APIä»•æ§˜æ›¸å®Œæˆ
- [ ] æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ
- [ ] é‹ç”¨ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒ

### æ¤œè¨¼æ–¹æ³•

```bash
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --all
cargo tarpaulin --out Html

# æ€§èƒ½ãƒ†ã‚¹ãƒˆ
cargo bench

# æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤
./scripts/deploy.ps1 -Package release.zip -Environment production
```

## æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã¸ã®å¼•ãç¶™ãäº‹é …

- åŸºæœ¬ç‰ˆãƒªãƒªãƒ¼ã‚¹å®Œäº†
- é‹ç”¨ä½“åˆ¶ç¢ºç«‹
- é«˜åº¦æ©Ÿèƒ½é–‹ç™ºæº–å‚™
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†é–‹å§‹

## ãƒªã‚¹ã‚¯ãƒ»èª²é¡Œ

- **ãƒ†ã‚¹ãƒˆæ™‚é–“ä¸è¶³**: å“è³ªä¿è¨¼æœŸé–“ã®ç¢ºä¿
- **æœ¬ç•ªç’°å¢ƒå•é¡Œ**: ç’°å¢ƒå·®ç•°ã«ã‚ˆã‚‹ä¸å…·åˆ
- **æ€§èƒ½è¦ä»¶**: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½æ¤œè¨¼

## å¯¾å¿œç­–

- æ—©æœŸãƒ†ã‚¹ãƒˆé–‹å§‹ãƒ»è‡ªå‹•åŒ–
- æœ¬ç•ªé¡ä¼¼ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆ
- æ®µéšçš„è² è·ãƒ†ã‚¹ãƒˆå®Ÿæ–½
